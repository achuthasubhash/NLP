{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=45rVF3t8OII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#like gpt3 no need to train model but do all things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yTBP_QYuu6tc"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TiU_ES5tzpMH"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "spkccRiv0CB3",
    "outputId": "8378c6c4-4823-4709-f6ff-443abaa86124"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartForSequenceClassification: ['model.encoder.version', 'model.decoder.version']\n",
      "- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xWiovVJG9ei_"
   },
   "source": [
    "We can use this pipeline by passing in a sequence and a list of candidate labels. The pipeline assumes by default that only one of the candidate labels is true, returning a list of scores for each label which add up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "hkfE6NRA0Dzy",
    "outputId": "bc2583cd-aae1-46f5-ad39-a0f0078477c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': ['politics', 'economics', 'public health'],\n",
       " 'scores': [0.972518801689148, 0.01458414364606142, 0.012897025793790817],\n",
       " 'sequence': 'Who are you voting for in 2020?'}"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = \"Who are you voting for in 2020?\"    #premise       #if 2 are related then get score\n",
    "candidate_labels = [\"politics\", \"public health\", \"economics\"] #hypotesis #check each and every hyp #here 3hyp\n",
    "\n",
    "classifier(sequence, candidate_labels) #0.97 is more suitable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PGXwxxyn9nOC"
   },
   "source": [
    "To do multi-class classification, simply pass `multi_class=True`. In this case, the scores will be independent, but each will fall between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "ZvZeVb2h5RX0",
    "outputId": "03b248fb-d08a-4699-f2bb-6ded805a7bb8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': ['politics', 'elections', 'public health', 'economics'],\n",
       " 'scores': [0.972069501876831,\n",
       "  0.967610776424408,\n",
       "  0.03248710557818413,\n",
       "  0.0061644683592021465],\n",
       " 'sequence': 'Who are you voting for in 2020?'}"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = \"Who are you voting for in 2020?\"\n",
    "candidate_labels = [\"politics\", \"public health\", \"economics\", \"elections\"]\n",
    "\n",
    "classifier(sequence, candidate_labels, multi_class=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lLLeDT1r9-yQ"
   },
   "source": [
    "Here's an example of sentiment classification: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "f7AF53Wl5f8W",
    "outputId": "fd1c8c8c-b68b-440e-bc94-0d3c7dd8d283"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': ['negative', 'positive'],\n",
       " 'scores': [0.9916268587112427, 0.00837317667901516],\n",
       " 'sequence': 'I hated this movie. The acting sucked.'}"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = \"I hated this movie. The acting sucked.\"\n",
    "candidate_labels = [\"positive\", \"negative\"]\n",
    "\n",
    "classifier(sequence, candidate_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uSoBpCpV6k4s"
   },
   "source": [
    "So how does this method work?\n",
    "\n",
    "The underlying model is trained on the task of Natural Language Inference (NLI), which takes in two sequences and determines whether they contradict each other, entail each other, or neither.\n",
    "\n",
    "This can be adapted to the task of zero-shot classification by treating the sequence which we want to classify as one NLI sequence (called the premise) and turning a candidate label into the other (the hypothesis). If the model predicts that the constructed premise _entails_ the hypothesis, then we can take that as a prediction that the label applies to the text. Check out [this blog post](https://joeddav.github.io/blog/2020/05/29/ZSL.html) for a more detailed explanation.\n",
    "\n",
    "By default, the pipeline turns labels into hypotheses with the template `This example is {class_name}.`. This works well in many settings, but you can also customize this for your specific setting. Let's add another review to our above sentiment classification example that's a bit more challenging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "5yLx3pRr5xQA",
    "outputId": "725212c9-4759-4457-ef4f-dc9b5ae20d84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'labels': ['negative', 'positive'],\n",
       "  'scores': [0.9916267991065979, 0.008373182266950607],\n",
       "  'sequence': 'I hated this movie. The acting sucked.'},\n",
       " {'labels': ['negative', 'positive'],\n",
       "  'scores': [0.8148515820503235, 0.1851484179496765],\n",
       "  'sequence': \"This movie didn't quite live up to my high expectations, but overall I still really enjoyed it.\"}]"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = [\n",
    "    \"I hated this movie. The acting sucked.\",\n",
    "    \"This movie didn't quite live up to my high expectations, but overall I still really enjoyed it.\"\n",
    "]\n",
    "candidate_labels = [\"positive\", \"negative\"]\n",
    "\n",
    "classifier(sequences, candidate_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CfrpyGWM782R"
   },
   "source": [
    "The second example is a bit harder. Let's see if we can improve the results by using a hypothesis template which is more specific to the setting of review sentiment analysis. Instead of the default, `This example is {}.`, we'll use, `The sentiment of this review is {}.` (where `{}` is replaced with the candidate class name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "kqx5hp7X8XNA",
    "outputId": "73a84935-de37-4f2c-abdc-46dacd6614ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'labels': ['negative', 'positive'],\n",
       "  'scores': [0.9890093207359314, 0.010990672744810581],\n",
       "  'sequence': 'I hated this movie. The acting sucked.'},\n",
       " {'labels': ['positive', 'negative'],\n",
       "  'scores': [0.9581228494644165, 0.0418771356344223],\n",
       "  'sequence': \"This movie didn't quite live up to my high expectations, but overall I still really enjoyed it.\"}]"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = [\n",
    "    \"I hated this movie. The acting sucked.\",\n",
    "    \"This movie didn't quite live up to my high expectations, but overall I still really enjoyed it.\"\n",
    "]\n",
    "candidate_labels = [\"positive\", \"negative\"]\n",
    "hypothesis_template = \"The sentiment of this review is {}.\" #telling that talking about sentiment so acuracy inc\n",
    "\n",
    "classifier(sequences, candidate_labels, hypothesis_template=hypothesis_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iArbRAe781-_"
   },
   "source": [
    "By providing a more precise hypothesis template, we are able to see a more accurate classification of the second review.\n",
    "\n",
    "> Note that sentiment classification is used here just as an illustrative example. The [Hugging Face Model Hub](https://huggingface.co/models?filter=text-classification) has a number of models trained specifically on sentiment tasks which can be used instead."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ðŸ¤— Zero Shot Pipeline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
